---
title: "Housing"
author: "Aakar Kale"
date: "28/10/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
getwd()
setwd("/Users/aakarkale/Desktop/CSUEB/Data Mining")
```



```{r}
housing = read.csv("BostonHousing.csv")
housing
```





##Sampling
```{r}
s <- sample(row.names(housing),5)
housing[s,]
```


##OverSampling
```{r}
s <- sample(row.names(housing),5,prob = ifelse(housing$RM>10,0.9,0.01))
housing[s,]
```



```{r}
names(housing) #Will give you all the variables/column-names
t(t(names(housing))) #Double Transpose.....used to print the columnnames in a usefull format
colnames(housing)[1] <- c("RENAMED COLUMNNAME") #This is how you rename column-names
```

Use model.matrix() to convert all categorical varibales in the dataframe into a set of dummy variables.
We must then turn the resulting data matrix back into a dataframe for further work.

```{r}
x <- model.matrix(~0 + RM + RAD,data = housing)
x
x <- as.data.frame(x)
x
```



Data Partitioning
- Partitioning into Training(60%) & Validation(40%) Data
```{r}

 
```





```{r}
# alternative, nicer plot (displayed) 
library(ggally)
ggpairs(housing[, c(1, 3, 12, 13)])
```




```{r}
install.packages("ggmap")
library(ggmap)
SCstudents <- read.csv("SC-US-students-GPS-data-2016.csv")
Map <- get_map("Denver, CO", zoom = 3)
ggmap(Map) + geom_point(aes(x = longitude, y = latitude), data = SCstudents,
alpha = 0.4, colour = "red", size = 0.5)
```





```{r}
install.packages("mosaic")
library(mosaic)
gdp.df <- read.csv("gdp.csv", skip = 4, stringsAsFactors = FALSE) names(gdp.df)[5] <- "GDP2015"
happiness.df <- read.csv("Veerhoven.csv")
# gdp map
mWorldMap(gdp.df, key = "Country.Name", fill = "GDP2015") + coord_map() # well-being map
mWorldMap(happiness.df, key = "Nation", fill = "Score") + coord_map() +
scale_fill_continuous(name = "Happiness")
```



```{r}
install.packages(ggplot2)
installed.packages()

install.packages("ggplot2",repos = c("http://rstudio.org/_packages","http://cran.rstudio.com"))
```



```{r}
summary(housing)
round(cor(housing),2)        #round is used to round-off the number to the 2nd decimal point
```





```{r}
housing[1:5,7]
#housing[rownumber,columnnumber] - format

```


#Chapter 2:- Overview of the Data Mining Process


This is the housing DATA that we are working on - 
```{r}
housing
```





The following chunk will help you in creating Training/Validation/Testing Partitons in your Data.
```{r}

set.seed(1)
train.rows <- sample(rownames(housing), dim(housing)[1]*0.6) 
# The code above means -- sample(data,selecting column using dim() and then multiplying it by 0.6 which means 60% of column 1)
train.data <- housing[train.rows, ] #This command is basically superposing the entire housing data on train.rows which will result in a dataframe creation by the name "train.data"

valid.rows <- setdiff(rownames(housing), train.rows) #The setdiff() command is a SETS command which chooses everything from the data except from the "train.rows"
valid.data <- housing[valid.rows, ]

# alternative code for ^^validation^^ (works only when row names are numeric): # collect all the columns without training row ID into validation setis:-   " valid.data <- housing[-train.rows, ] " using the "-" sign will make sure it does the same thing as above.

# Just in case if you also want to add a TEST PARTITION in your data you would execute the following command.........  
#   test.rows <- setdiff(rownames(housing), union(train.rows, valid.rows))
#   test.data <- housing[test.rows, ]

```




#Chapter 3:- Data Visualization

1.Histograms
2.Box Plots
3.Bar Graphs
4.HeatMaps


```{r}
## simple heatmap of correlations (without values) 
heatmap(cor(housing), Rowv = NA, Colv = NA)


install.packages("reshape")
library(ggplot2)
library(reshape) # to generate input for the plot
cor.mat <- round(cor(housing),2) # rounded correlation matrix 
melted.cor.mat <- melt(cor.mat)
ggplot(melted.cor.mat, aes(x = X1, y = X2, fill = value)) +
  geom_tile() + 
  geom_text(aes(x = X1, y = X2, label = value))




#The command below gives you a heatmap of missing data
heatmap(1 * is.na(housing), Rowv = NA, Colv = NA)


```





#Full of Errors......

```{r}
## color plot
par(xpd=TRUE) # allow legend to be displayed outside of plot area 
plot(housing$NOX ~ housing$LSTAT, ylab = "NOX", xlab = "LSTAT",col = ifelse(housing$CAT..MEDV == 1, "black", "gray"))
# add legend outside of plotting area
# In legend() use argument inset = to control the location of the legend relative # to the plot.
legend("topleft", inset=c(0, -0.2),legend = c("CAT.MEDV = 1", "CAT.MEDV = 0"), col = c("black", "gray"),pch = 1, cex = 0.5)
# alternative plot with ggplot
library(ggplot2)
ggplot(housing, aes(y = NOX, x = LSTAT, colour= CAT..MEDV)) +
geom_point(alpha = 0.6)
## panel plots
# compute mean MEDV per RAD and CHAS
# In aggregate() use argument drop = FALSE to include all combinations
# (exiting and missing) of RAD X CHAS.
data.for.plot <- aggregate(housing$MEDV, by = list(housing$RAD, housing$CHAS),FUN = mean, drop = FALSE) names(data.for.plot) <- c("RAD", "CHAS", "meanMEDV")
# plot the data
par(mfcol = c(2,1))
barplot(height = data.for.plot$meanMEDV[data.for.plot$CHAS == 0],
names.arg = data.for.plot$RAD[data.for.plot$CHAS == 0],
xlab = "RAD", ylab = "Avg. MEDV", main = "CHAS = 0") barplot(height = data.for.plot$meanMEDV[data.for.plot$CHAS == 1],
names.arg = data.for.plot$RAD[data.for.plot$CHAS == 1],
xlab = "RAD", ylab = "Avg. MEDV", main = "CHAS = 1") # alternative plot with ggplot
ggplot(data.for.plot) +
geom_bar(aes(x = as.factor(RAD), y = `meanMEDV`), stat = "identity") + xlab("RAD") + facet_grid(CHAS ~ .)
    NOX0.4 0.5 0.6 0.7 0.8
```


#Simple Plot

```{r}
# use plot() to generate a matrix of 4X4 panels with variable name on the diagonal, 
# and scatter plots in the remaining panels.

plot(housing[, c(1, 3, 12, 13)])


# ALTERNATIVE, nicer plot (displayed)

#install.packages("GGally")
library(GGally)
ggpairs(housing[, c(1, 3, 12, 13)])

```



#RESCALING to view the visualization in a better way.

```{r}

## scatter plot: regular and log scale
plot(housing$MEDV ~ housing$CRIM, xlab = "CRIM", ylab = "MEDV")
# to use logarithmic scale set argument log = to either 'x', 'y', or 'xy'. 
plot(housing$MEDV ~ housing$CRIM,xlab = "CRIM", ylab = "MEDV", log = 'xy')



# ALTERNATIVE log-scale plot with ggplot
library(ggplot2)
ggplot(housing) + geom_point(aes(x = CRIM, y = MEDV)) + scale_x_log10(breaks = 10^(-2:2),
labels = format(10^(-2:2), scientific = FALSE, drop0trailing = TRUE)) +
scale_y_log10(breaks = c(5, 10, 20, 40))
```


#See and learn how to draw this graph

Read about:-
1. aes
2. geom_line()
3. geaom_smooth()
  
#GGPLOT IS CERTAINLY ONE OF THE MOST IMPORTANT LIBRARIES IN R.

```{r}
# alternative plot with ggplot
library(ggplot2)
Amtrak.df <- read.csv("Amtrak.csv")
ggplot(Amtrak.df, aes(y = Ridership, x = Month, group = 12)) +
geom_line() + geom_smooth(formula = y ~ poly(x, 2), method= "lm", colour = "red", se = FALSE, na.rm = TRUE)
```



```{r}
#install.packages("scales")
library(scales)
universal.df <- read.csv("UniversalBank.csv")
plot(jitter(universal.df$CCAvg, 1) ~ jitter(universal.df$Income, 1),
col = alpha(ifelse(universal.df$Securities.Account == 0, "gray", "black"), 0.4), pch = 20, log = 'xy', ylim = c(0.1, 10),
xlab = "Income", ylab = "CCAvg")

# ALTERNATIVE with ggplot

library(ggplot2) 
ggplot(universal.df) +
geom_jitter(aes(x = Income, y = CCAvg, colour = Securities.Account)) + scale_x_log10(breaks = c(10, 20, 40, 60, 100, 200)) + scale_y_log10(breaks = c(0.1, 0.2, 0.4, 0.6, 1.0, 2.0, 4.0, 6.0))


```



#NETWORK GRAPH

Understand the Graph properly
EACH and every line of code


```{r}
#install.packages("igraph")
library(igraph)
ebay.df <- read.csv("eBayNetwork.csv")
# transform node ids to factors 
ebay.df[,1] <- as.factor(ebay.df[,1]) 
ebay.df[,2] <- as.factor(ebay.df[,2])
graph.edges <- as.matrix(ebay.df[,1:2])
g <- graph.edgelist(graph.edges, directed = FALSE) 
isBuyer <- V(g)$name %in% graph.edges[,2]
plot(g, vertex.label = NA, vertex.color = ifelse(isBuyer, "gray", "black"), vertex.size = ifelse(isBuyer, 7, 10))
```


#TreeMaps

```{r}
install.packages("treemap")
library(treemap)
tree.df <- read.csv("EbayTreemap.csv")
# add column for negative feedback
tree.df$negative.feedback <- 1* (tree.df$Seller.Feedback < 0)
# draw treemap
treemap(tree.df, index = c("Category","Sub.Category", "Brand"),
vSize = "High.Bid", vColor = "negative.feedback", fun.aggregate = "mean", align.labels = list(c("left", "top"), c("right", "bottom"), c("center", "center")), palette = rev(gray.colors(3)), type = "manual", title = "")
```




```{r}
library(ggmap)
SCstudents <- read.csv("SC-US-students-GPS-data-2016.csv")

#In your GCP Console the API Key that you need to use for Google Maps in R is "Maps Static API"....Make sure you disable it once done using because it is NOT for FREE.

register_google(key = "AIzaSyDDLm-37lvXKv2ItpvPJDMm5tETM9tkOPo", write = TRUE)   
Map <- get_map("Oklahoma City,OK", zoom = 4)
ggmap(Map) + geom_point(aes(x = longitude, y = latitude), data = SCstudents,alpha = 0.5, colour = "red", size = 0.8)
```


```{r}
library(mosaic)
#install.packages("lattice")
#install.packages("ggformula")
#install.packages("ggstance")
#install.packages("mapproj")

gdp.df <- read.csv("gdp.csv", skip = 4, stringsAsFactors = FALSE)
names(gdp.df)[5] <- "GDP2015"
happiness.df <- read.csv("Veerhoven.csv")
# gdp map
mWorldMap(gdp.df, key = "Country.Name", fill = "GDP2015") + coord_map()
# eell-being map
mWorldMap(happiness.df, key = "Nation", fill = "Score") + coord_map() +
scale_fill_continuous(name = "Happiness")
```


Prediction
• Plot outcome on the y-axis of boxplots, bar charts, and scatter plots.
• Study relation of outcome to categorical predictors via side-by-side box-
plots, bar charts, and multiple panels.
• Study relation of outcome to numerical predictors via scatter plots.
• Use distribution plots (boxplot, histogram) for determining needed trans- formations of the outcome variable (and/or numerical predictors).
• Examine scatter plots with added color/panels/size to determine the need for interaction terms.
• Use various aggregation levels and zooming to determine areas of the data with different behavior, and to evaluate the level of global vs. local patterns.


Classification
• Study relation of outcome to categorical predictors using bar charts with the outcome on the y-axis.
• Studyrelationofoutcometopairsofnumericalpredictorsviacolor-coded scatter plots (color denotes the outcome).
• Study relation of outcome to numerical predictors via side-by-side box- plots: Plot boxplots of a numerical variable by outcome. Create similar displays for each numerical predictor. The most separable boxes indicate potentially useful predictors.
• Use color to represent the outcome variable on a parallel coordinate plot.
• Use distribution plots (boxplot, histogram) for determining needed trans- formations of numerical predictor variables.
• Examine scatter plots with added color/panels/size to determine the need for interaction terms.
• Use various aggregation levels and zooming to determine areas of the data with different behavior, and to evaluate the level of global vs. local patterns.


Time Series Forecasting
• Create line graphs at different temporal aggregations to determine types of patterns.
• Use zooming and panning to examine various shorter periods of the series to determine areas of the data with different behavior.
• Use various aggregation levels to identify global and local patterns.
• Identify missing values in the series (that will require handling).
• Overlay trend lines of different types to determine adequate modeling choices.


Unsupervised Learning
• Create scatter plot matrices to identify pairwise relationships and cluster- ing of observations.
• Use heatmaps to examine the correlation table.
• Use various aggregation levels and zooming to determine areas of the data with different behavior.
• Generate a parallel coordinates plot to identify clusters of observations.





```{r}
heatmap(cor(housing))
```


#Principal Component Analysis



```{r}
cereals.df <- read.csv("Cereals.csv")
# compute PCs on two dimensions
pcs <- prcomp(data.frame(cereals.df$calories, cereals.df$rating)) 
summary(pcs)
pcs$rot
scores <- pcs$x
head(scores, 5)
```



#Evaluating Predictive Performance
In this chapter we will evaluate how data mining methods can be assessed on the basis of the following Prediction Accuracy Measures:-
1. Avreage Error
2. MAPE
3. RMSE
4. MPE
5. RMSE



Shown below is how to do Performance Evaluation:- 

```{r}
# package forecast is required to evaluate performance
library(forecast)
# load file
toyota.corolla.df <- read.csv("ToyotaCorolla.csv")

# randomly generate training and validation sets
training <- sample(toyota.corolla.df$Id, 600)
validation <- sample(setdiff(toyota.corolla.df$Id, training), 400)

# run linear regression model
reg <- lm(Price~., data=toyota.corolla.df[,-c(1,2,8,11)], subset=training,na.action=na.exclude)
pred_t <- predict(reg, na.action=na.pass)
pred_v <- predict(reg, newdata=toyota.corolla.df[validation,-c(1,2,8,11)],na.action=na.pass)

# Performance Evaluation

# training
accuracy(pred_t, toyota.corolla.df[training,]$Price) 
# validation
accuracy(pred_v, toyota.corolla.df[validation,]$Price)
```



```{r}

```



```{r}

```


















