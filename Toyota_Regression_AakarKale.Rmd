---
title: "ToyotaCorolla_regression"
author: "Arkady Khodursky"
date: "10/18/2019"
output:
  pdf_document: default
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




```{r}
tinytex::install_tinytex()
```



### Input data, choose predictors
```{r}
car.df <- read.csv("ToyotaCorolla.csv")
car.df <- car.df[1:1000, ]  # This is to use first 1000 rows of data
View(car.df)
t(t(names(car.df)))  #The names command is used as a generic accessor function, and is transposed twice in order to set it as a column again

selected.var <- c(3, 4, 7, 8, 9, 10, 12, 13, 14, 17, 18) #This command helps us to select the variables we require for regression.
```

This chunk was mainly to choose our predictors for regression.


### Partition the data
```{r}
set.seed(1)  # set seed for reproducing the partition
train.index <- sample(c(1:1000), 600)   #selecting the training partition
head(train.index)
train.df <- car.df[train.index, selected.var] #Here we are creating the training partition by selecting 60% of the data
valid.df <- car.df[-train.index, selected.var] #Here we are creating the validation partition by selecting the compliment of the 60% of the data, which is the remaining 40%.

head(valid.df)
head(train.df)
```
This chunk was to partition the data in Training and Validation Partition

### Run model
```{r}

# use lm() to run a linear regression of Price on all 11 predictors in the training set.
# use . after ~ to include all the remaining columns in train.df as predictors.

car.lm <- lm(Price ~ ., data = train.df)      # Here we use lm() to run a linear regression of price on all 11 predictors in the training set, we use the . after ~ to select all the reamining columns in the train.df as predictors
options(scipen = 999)
summary(car.lm)
```
This chunk was to run the linear regression model so that we can make predictions based on the model


### Make predictions on a hold-out set
```{r}

library(forecast)

# use predict() to make predictions on a new set. 
car.lm.pred <- predict(car.lm, valid.df)     #we use the predict() function to make predictions on a new set (validation set)

options(scipen=999, digits = 0)
some.residuals <- valid.df$Price[1:20] - car.lm.pred[1:20]
data.frame("Predicted" = car.lm.pred[1:20], "Actual" = valid.df$Price[1:20],"Residual" = some.residuals)
options(scipen=999, digits = 3)

accuracy(car.lm.pred, valid.df$Price) # here accuracy() is used to compute common accuracy measures.Returns range of summary measures of the forecast accuracy.
```



###Histogram of residuals
```{r}
library(forecast)
car.lm.pred <- predict(car.lm, valid.df)
all.residuals <- valid.df$Price - car.lm.pred
length(all.residuals[which(all.residuals > -2000 & all.residuals < 2000)])/400
hist(all.residuals, breaks = 25, xlab = "Residuals", main = "") #The generic function hist computes a histogram of the given data values. 

```
In this chunk we created the histogram of residuals.



### Run  an exhaustive search for the best model
```{r}
# use regsubsets() in package leaps to run an exhaustive search. 
# unlike with lm, categorical predictors must be turned into dummies manually.

# create dummies for fuel type

train.df <- car.df[train.index, selected.var]
valid.df <- car.df[-train.index, selected.var]
train.index <- sample(c(1:1000), 600)  
train.df <- car.df[train.index, selected.var]

dim(train.df)
Fuel_Type1 <- as.data.frame(model.matrix(~ 0 + Fuel_Type, data=train.df)) #model.matrix here creates a matrix, by expanding factors to a set of dummy variables.


train.df <- cbind(train.df[,-4], Fuel_Type1[,]) #replace Fuel_Type column with 2 dummies created earlier by using model.matrix() function
head(train.df)

Fuel_Type2 <- as.data.frame(model.matrix(~ 0 + Fuel_Type, data=valid.df))
# replace Fuel_Type column with 2 dummies
valid.df <- cbind(valid.df[,-4], Fuel_Type2[,])
head(valid.df)
dim(valid.df)

#install.packages("leaps")
library(leaps)
#leaps() performs an exhaustive search for the best subsets of the variables in x for predicting y in linear regression, using an efficient branch-and-bound algorithm.

# regsubsets() is used to run an exhaustive search, model selection by exhaustive search, forward or backward stepwise
search <- regsubsets(Price ~ ., data = train.df, nbest = 1, nvmax = dim(train.df)[2],method = "exhaustive")

sum <- summary(search)

# show models
sum$which

# show metrics
sum$rsq
sum$adjr2
sum$cp
```

In this chunk we achieved the best model to run using the regsubsets()




# use step() to run stepwise regression, backward selection.
```{r}
head(valid.df)
head(train.df)
car.lm <- lm(Price ~ ., data = train.df)
car.lm.step <- step(car.lm, direction = "backward")   #Here step() is used to run stepwise regression using backward selection
summary(car.lm.step) 
#Which variables did it drop?
#Met_Color & Fuel_TypePetrol
car.lm.step.pred <- predict(car.lm.step, valid.df)
accuracy(car.lm.step.pred, valid.df$Price)
```





## Forward selection
```{r}
car.lm <- lm(Price ~ ., data = train.df)
car.lm.step <- step(car.lm, direction = "forward")   #Here step() is used to run stepwise regression using forward selection
summary(car.lm.step) #this gives a summary of all the data

```





#Stepwise Regression in Both Directions
```{r}
# use step() to run stepwise regression.
car.lm <- lm(Price ~ ., data = train.df)
car.lm.step <- step(car.lm, direction = "both")    #Here step() is used to run stepwise regression using both the selections
summary(car.lm.step)
# Which variables were dropped/added?
#Met_Color & Fuel_TypePetrol
car.lm.step.pred <- predict(car.lm.step, valid.df)
accuracy(car.lm.step.pred, valid.df$Price)
```

